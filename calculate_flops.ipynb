{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [7]\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "DISTRIBUTED_STRATEGY = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.NcclAllReduce(),\n",
    "    devices=['/gpu:%d' % index for index in range(len(SELECTED_GPUS))]\n",
    ")\n",
    "\n",
    "NUM_GPUS = DISTRIBUTED_STRATEGY.num_replicas_in_sync\n",
    "\n",
    "print('Number of devices: {}'.format(NUM_GPUS))\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from skimage import transform\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "from vit_keras import vit\n",
    "from vit_keras.layers import ClassToken, AddPositionEmbs, MultiHeadSelfAttention, TransformerBlock\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "PATCH_SIZE = 16\n",
    "HIDDEN_DIM = 768\n",
    "MLP_DIM = 3072\n",
    "CHANNELS_MLP_DIM = 3072\n",
    "TOKENS_MLP_DIM = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops(model):\n",
    "    \"\"\"\n",
    "    from https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-768977280\n",
    "    \"\"\"\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/leondgarse/Keras_mlp/blob/main/res_mlp.py\n",
    "\n",
    "def channel_affine(inputs, use_bias=True, weight_init_value=1, name=''):\n",
    "    ww_init = tfkeras.initializers.Constant(weight_init_value) if weight_init_value != 1 else 'ones'\n",
    "    nn = tf.keras.backend.expand_dims(inputs, 1)\n",
    "    nn = tf.keras.layers.DepthwiseConv2D(1, depthwise_initializer=ww_init, use_bias=use_bias, name=name + 'affine')(nn)\n",
    "    return tf.keras.backend.squeeze(nn, 1)\n",
    "\n",
    "def mlp_block(inputs, mlp_dim, activation='gelu', name=''):\n",
    "    affine_inputs = channel_affine(inputs, use_bias=True, name=name + '1_')\n",
    "    nn = tf.keras.layers.Permute((2, 1), name=name + 'permute_1')(affine_inputs)\n",
    "    nn = tf.keras.layers.Dense(nn.shape[-1], name=name + 'dense_1')(nn)\n",
    "    nn = tf.keras.layers.Permute((2, 1), name=name + 'permute_2')(nn)\n",
    "    nn = channel_affine(nn, use_bias=False, name=name + '1_gamma_')\n",
    "    skip_conn = tf.keras.layers.Add(name=name + 'add_1')([nn, affine_inputs])\n",
    "\n",
    "    affine_skip = channel_affine(skip_conn, use_bias=True, name=name + '2_')\n",
    "    nn = tf.keras.layers.Dense(mlp_dim, name=name + 'dense_2_1')(affine_skip)\n",
    "    nn = tf.keras.layers.Activation(activation, name=name + 'gelu')(nn)\n",
    "    nn = tf.keras.layers.Dense(inputs.shape[-1], name=name + 'dense_2_2')(nn)\n",
    "    nn = channel_affine(nn, use_bias=False, name=name + '2_gamma_')\n",
    "    nn = tf.keras.layers.Add(name=name + 'add_2')([nn, affine_skip])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/Benjamin-Etheredge/mlp-mixer-keras/blob/main/mlp_mixer_keras/mlp_mixer.py\n",
    "\n",
    "class MlpBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, activation=None, **kwargs):\n",
    "        super(MlpBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = tf.keras.activations.gelu\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        self.dense2 = tf.keras.layers.Dense(dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_signature):\n",
    "        return (input_signature[0], self.dim)\n",
    "\n",
    "class MixerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        channel_dim,\n",
    "        token_mixer_hidden_dim,\n",
    "        channel_mixer_hidden_dim=None,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MixerBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = tf.keras.activations.gelu\n",
    "\n",
    "        if channel_mixer_hidden_dim is None:\n",
    "            channel_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "        self.permute1 = tf.keras.layers.Permute((2, 1))\n",
    "        self.token_mixer = MlpBlock(num_patches, token_mixer_hidden_dim, name='token_mixer')\n",
    "\n",
    "        self.permute2 = tf.keras.layers.Permute((2, 1))\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "        self.channel_mixer = MlpBlock(channel_dim, channel_mixer_hidden_dim, name='channel_mixer')\n",
    "\n",
    "        self.skip_connection1 = tf.keras.layers.Add()\n",
    "        self.skip_connection2 = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        skip_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.permute1(x)\n",
    "        x = self.token_mixer(x)\n",
    "\n",
    "        x = self.permute2(x)\n",
    "\n",
    "        x = self.skip_connection1([x, skip_x])\n",
    "        skip_x = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.channel_mixer(x)\n",
    "\n",
    "        x = self.skip_connection2([x, skip_x])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch_id(branch_number):\n",
    "    if branch_number == 1:\n",
    "        return 'transformer_block'\n",
    "    else:\n",
    "        return 'transformer_block_%d' % (branch_number - 1)\n",
    "\n",
    "def get_model(dataset_name, branch_type, branch_number):\n",
    "    if dataset_name == 'disco':\n",
    "        model_file_name = 'vit_cc_backbone_v2.h5'\n",
    "        output_units = 1\n",
    "        output_activation = None\n",
    "    elif dataset_name == 'fashion_mnist':\n",
    "        model_file_name = 'vit_fashion_mnist_v1.h5'\n",
    "        output_units = 10\n",
    "        output_activation = 'softmax'\n",
    "    elif dataset_name == 'cifar10':\n",
    "        model_file_name = 'vit_cifar10_v1.h5'\n",
    "        output_units = 10\n",
    "        output_activation = 'softmax'\n",
    "    else:\n",
    "        model_file_name = 'vit_cifar100_v1.h5'\n",
    "        output_units = 100\n",
    "        output_activation = 'softmax'\n",
    "\n",
    "    backbone_model = tf.keras.models.load_model(model_file_name, custom_objects={\n",
    "        'ClassToken': ClassToken,\n",
    "        'AddPositionEmbs': AddPositionEmbs,\n",
    "        'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "        'TransformerBlock': TransformerBlock,\n",
    "    })\n",
    "    \n",
    "    # freeze\n",
    "    for layer in backbone_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    if branch_type == 'mlp':\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        y = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"Transformer/encoder_norm\"\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(y)\n",
    "\n",
    "    elif branch_type == 'vit':\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        y, _ = TransformerBlock(\n",
    "            num_heads=12,\n",
    "            mlp_dim=3072,\n",
    "            dropout=0.1,\n",
    "            name=f\"Transformer/encoderblock_x\",\n",
    "        )(y)\n",
    "        y = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=\"Transformer/encoder_norm\"\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(y)\n",
    "\n",
    "    elif branch_type.startswith('cnn_'):\n",
    "        y0, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        channels = HIDDEN_DIM\n",
    "        width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "        y1 = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken')(y0)\n",
    "        y1 = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape')(y1)\n",
    "        y2 = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken')(y0)\n",
    "        y2 = tf.keras.layers.RepeatVector(width * height)(y2)\n",
    "        y2 = tf.keras.layers.Reshape((width, height, channels), name='cls_reshape')(y2)\n",
    "        if branch_type == 'cnn_ignore':\n",
    "            y = y1\n",
    "        elif branch_type == 'cnn_add':\n",
    "            y = tf.keras.layers.Add()([y1, y2])\n",
    "        elif branch_type == 'cnn_project':\n",
    "            y = tf.keras.layers.Concatenate()([y1, y2])\n",
    "        y = tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation='elu',\n",
    "            padding='same'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "        y = tf.keras.layers.Flatten()(y)\n",
    "\n",
    "    elif branch_type == 'resmlp':\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        y = mlp_block(y, mlp_dim=MLP_DIM, name='mlp_mixer')\n",
    "        y = tf.keras.layers.GlobalAveragePooling1D()(y)\n",
    "\n",
    "    elif branch_type == 'mlp_mixer':\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        num_patches = (IMAGE_SIZE // PATCH_SIZE) ** 2 + 1\n",
    "        y = MixerBlock(\n",
    "            num_patches=num_patches,\n",
    "            channel_dim=HIDDEN_DIM,\n",
    "            token_mixer_hidden_dim=TOKENS_MLP_DIM,\n",
    "            channel_mixer_hidden_dim=CHANNELS_MLP_DIM\n",
    "        )(y)\n",
    "        y = tf.keras.layers.GlobalAveragePooling1D()(y)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Unknown branch type: %s' % branch_type)\n",
    "    \n",
    "    # MLP head\n",
    "    initializer = tf.keras.initializers.he_normal()\n",
    "    regularizer = tf.keras.regularizers.l2()\n",
    "    y = tf.keras.layers.Dense(\n",
    "        units=256,\n",
    "        activation='elu',\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=regularizer\n",
    "    )(y)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(\n",
    "        units=256,\n",
    "        activation='elu',\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=regularizer\n",
    "    )(y)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(\n",
    "        units=output_units,\n",
    "        activation=output_activation,\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=regularizer\n",
    "    )(y)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=backbone_model.get_layer(index=0).input,\n",
    "        outputs=y\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_types = [\n",
    "    'mlp',\n",
    "    'vit',\n",
    "    'cnn_ignore',\n",
    "    'cnn_add',\n",
    "    'cnn_project',\n",
    "    'resmlp',\n",
    "    'mlp_mixer',\n",
    "]\n",
    "\n",
    "dataset_names = [\n",
    "    'cifar10',\n",
    "    'cifar100',\n",
    "    'disco',\n",
    "    'fashion_mnist',\n",
    "]\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for branch_type in branch_types:\n",
    "        flops = []\n",
    "        for branch_number in range(1, 12):\n",
    "            tf.keras.backend.clear_session()\n",
    "            flops.append(get_flops(get_model(dataset_name, branch_type, branch_number)) / 10 ** 9)\n",
    "        print('###', dataset_name, branch_type)\n",
    "        print(flops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
