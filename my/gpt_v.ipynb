{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # Shape: [batch_size, embedding_dim, num_patches^(0.5), num_patches^(0.5)]\n",
    "        x = x.flatten(2)  # Flatten height and width\n",
    "        x = x.transpose(1, 2)  # Shape: [batch_size, num_patches, embedding_dim]\n",
    "        return x\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_patches):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, num_patches, embedding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_encoding\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embedding_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x\n",
    "\n",
    "# Vision Transformer Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10, \n",
    "                 embedding_dim=128, num_heads=8, mlp_dim=256, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dim, self.patch_embed.num_patches)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embedding_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling (for classification)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1, Batch 100, Loss: 2.101\n",
      "Epoch 1, Batch 200, Loss: 1.939\n",
      "Epoch 1, Batch 300, Loss: 1.870\n",
      "Epoch 1, Batch 400, Loss: 1.786\n",
      "Epoch 1, Batch 500, Loss: 1.737\n",
      "Epoch 1, Batch 600, Loss: 1.685\n",
      "Epoch 1, Batch 700, Loss: 1.658\n",
      "Epoch 2, Batch 100, Loss: 1.571\n",
      "Epoch 2, Batch 200, Loss: 1.540\n",
      "Epoch 2, Batch 300, Loss: 1.528\n",
      "Epoch 2, Batch 400, Loss: 1.530\n",
      "Epoch 2, Batch 500, Loss: 1.507\n",
      "Epoch 2, Batch 600, Loss: 1.458\n",
      "Epoch 2, Batch 700, Loss: 1.465\n",
      "Epoch 3, Batch 100, Loss: 1.416\n",
      "Epoch 3, Batch 200, Loss: 1.400\n",
      "Epoch 3, Batch 300, Loss: 1.351\n",
      "Epoch 3, Batch 400, Loss: 1.379\n",
      "Epoch 3, Batch 500, Loss: 1.371\n",
      "Epoch 3, Batch 600, Loss: 1.345\n",
      "Epoch 3, Batch 700, Loss: 1.334\n",
      "Epoch 4, Batch 100, Loss: 1.283\n",
      "Epoch 4, Batch 200, Loss: 1.306\n",
      "Epoch 4, Batch 300, Loss: 1.268\n",
      "Epoch 4, Batch 400, Loss: 1.291\n",
      "Epoch 4, Batch 500, Loss: 1.251\n",
      "Epoch 4, Batch 600, Loss: 1.302\n",
      "Epoch 4, Batch 700, Loss: 1.237\n",
      "Epoch 5, Batch 100, Loss: 1.253\n",
      "Epoch 5, Batch 200, Loss: 1.235\n",
      "Epoch 5, Batch 300, Loss: 1.235\n",
      "Epoch 5, Batch 400, Loss: 1.186\n",
      "Epoch 5, Batch 500, Loss: 1.205\n",
      "Epoch 5, Batch 600, Loss: 1.209\n",
      "Epoch 5, Batch 700, Loss: 1.205\n",
      "Epoch 6, Batch 100, Loss: 1.163\n",
      "Epoch 6, Batch 200, Loss: 1.156\n",
      "Epoch 6, Batch 300, Loss: 1.176\n",
      "Epoch 6, Batch 400, Loss: 1.186\n",
      "Epoch 6, Batch 500, Loss: 1.159\n",
      "Epoch 6, Batch 600, Loss: 1.174\n",
      "Epoch 6, Batch 700, Loss: 1.156\n",
      "Epoch 7, Batch 100, Loss: 1.120\n",
      "Epoch 7, Batch 200, Loss: 1.140\n",
      "Epoch 7, Batch 300, Loss: 1.128\n",
      "Epoch 7, Batch 400, Loss: 1.116\n",
      "Epoch 7, Batch 500, Loss: 1.103\n",
      "Epoch 7, Batch 600, Loss: 1.110\n",
      "Epoch 7, Batch 700, Loss: 1.102\n",
      "Epoch 8, Batch 100, Loss: 1.066\n",
      "Epoch 8, Batch 200, Loss: 1.046\n",
      "Epoch 8, Batch 300, Loss: 1.046\n",
      "Epoch 8, Batch 400, Loss: 1.061\n",
      "Epoch 8, Batch 500, Loss: 1.060\n",
      "Epoch 8, Batch 600, Loss: 1.067\n",
      "Epoch 8, Batch 700, Loss: 1.080\n",
      "Epoch 9, Batch 100, Loss: 0.986\n",
      "Epoch 9, Batch 200, Loss: 1.046\n",
      "Epoch 9, Batch 300, Loss: 1.037\n",
      "Epoch 9, Batch 400, Loss: 1.014\n",
      "Epoch 9, Batch 500, Loss: 1.002\n",
      "Epoch 9, Batch 600, Loss: 1.010\n",
      "Epoch 9, Batch 700, Loss: 1.029\n",
      "Epoch 10, Batch 100, Loss: 0.959\n",
      "Epoch 10, Batch 200, Loss: 0.980\n",
      "Epoch 10, Batch 300, Loss: 0.987\n",
      "Epoch 10, Batch 400, Loss: 0.998\n",
      "Epoch 10, Batch 500, Loss: 0.989\n",
      "Epoch 10, Batch 600, Loss: 0.985\n",
      "Epoch 10, Batch 700, Loss: 0.967\n",
      "Epoch 11, Batch 100, Loss: 0.951\n",
      "Epoch 11, Batch 200, Loss: 0.936\n",
      "Epoch 11, Batch 300, Loss: 0.942\n",
      "Epoch 11, Batch 400, Loss: 0.942\n",
      "Epoch 11, Batch 500, Loss: 0.966\n",
      "Epoch 11, Batch 600, Loss: 0.935\n",
      "Epoch 11, Batch 700, Loss: 0.947\n",
      "Epoch 12, Batch 100, Loss: 0.896\n",
      "Epoch 12, Batch 200, Loss: 0.916\n",
      "Epoch 12, Batch 300, Loss: 0.913\n",
      "Epoch 12, Batch 400, Loss: 0.903\n",
      "Epoch 12, Batch 500, Loss: 0.931\n",
      "Epoch 12, Batch 600, Loss: 0.909\n",
      "Epoch 12, Batch 700, Loss: 0.945\n",
      "Epoch 13, Batch 100, Loss: 0.864\n",
      "Epoch 13, Batch 200, Loss: 0.875\n",
      "Epoch 13, Batch 300, Loss: 0.890\n",
      "Epoch 13, Batch 400, Loss: 0.866\n",
      "Epoch 13, Batch 500, Loss: 0.890\n",
      "Epoch 13, Batch 600, Loss: 0.901\n",
      "Epoch 13, Batch 700, Loss: 0.897\n",
      "Epoch 14, Batch 100, Loss: 0.833\n",
      "Epoch 14, Batch 200, Loss: 0.851\n",
      "Epoch 14, Batch 300, Loss: 0.838\n",
      "Epoch 14, Batch 400, Loss: 0.890\n",
      "Epoch 14, Batch 500, Loss: 0.876\n",
      "Epoch 14, Batch 600, Loss: 0.874\n",
      "Epoch 14, Batch 700, Loss: 0.887\n",
      "Epoch 15, Batch 100, Loss: 0.817\n",
      "Epoch 15, Batch 200, Loss: 0.811\n",
      "Epoch 15, Batch 300, Loss: 0.850\n",
      "Epoch 15, Batch 400, Loss: 0.819\n",
      "Epoch 15, Batch 500, Loss: 0.853\n",
      "Epoch 15, Batch 600, Loss: 0.858\n",
      "Epoch 15, Batch 700, Loss: 0.835\n",
      "Epoch 16, Batch 100, Loss: 0.778\n",
      "Epoch 16, Batch 200, Loss: 0.802\n",
      "Epoch 16, Batch 300, Loss: 0.793\n",
      "Epoch 16, Batch 400, Loss: 0.831\n",
      "Epoch 16, Batch 500, Loss: 0.839\n",
      "Epoch 16, Batch 600, Loss: 0.852\n",
      "Epoch 16, Batch 700, Loss: 0.851\n",
      "Epoch 17, Batch 100, Loss: 0.743\n",
      "Epoch 17, Batch 200, Loss: 0.752\n",
      "Epoch 17, Batch 300, Loss: 0.772\n",
      "Epoch 17, Batch 400, Loss: 0.801\n",
      "Epoch 17, Batch 500, Loss: 0.795\n",
      "Epoch 17, Batch 600, Loss: 0.807\n",
      "Epoch 17, Batch 700, Loss: 0.852\n",
      "Epoch 18, Batch 100, Loss: 0.721\n",
      "Epoch 18, Batch 200, Loss: 0.764\n",
      "Epoch 18, Batch 300, Loss: 0.780\n",
      "Epoch 18, Batch 400, Loss: 0.780\n",
      "Epoch 18, Batch 500, Loss: 0.778\n",
      "Epoch 18, Batch 600, Loss: 0.783\n",
      "Epoch 18, Batch 700, Loss: 0.802\n",
      "Epoch 19, Batch 100, Loss: 0.715\n",
      "Epoch 19, Batch 200, Loss: 0.725\n",
      "Epoch 19, Batch 300, Loss: 0.743\n",
      "Epoch 19, Batch 400, Loss: 0.769\n",
      "Epoch 19, Batch 500, Loss: 0.749\n",
      "Epoch 19, Batch 600, Loss: 0.766\n",
      "Epoch 19, Batch 700, Loss: 0.779\n",
      "Epoch 20, Batch 100, Loss: 0.678\n",
      "Epoch 20, Batch 200, Loss: 0.707\n",
      "Epoch 20, Batch 300, Loss: 0.721\n",
      "Epoch 20, Batch 400, Loss: 0.741\n",
      "Epoch 20, Batch 500, Loss: 0.733\n",
      "Epoch 20, Batch 600, Loss: 0.744\n",
      "Epoch 20, Batch 700, Loss: 0.764\n",
      "Epoch 21, Batch 100, Loss: 0.668\n",
      "Epoch 21, Batch 200, Loss: 0.686\n",
      "Epoch 21, Batch 300, Loss: 0.693\n",
      "Epoch 21, Batch 400, Loss: 0.716\n",
      "Epoch 21, Batch 500, Loss: 0.728\n",
      "Epoch 21, Batch 600, Loss: 0.731\n",
      "Epoch 21, Batch 700, Loss: 0.761\n",
      "Epoch 22, Batch 100, Loss: 0.679\n",
      "Epoch 22, Batch 200, Loss: 0.649\n",
      "Epoch 22, Batch 300, Loss: 0.685\n",
      "Epoch 22, Batch 400, Loss: 0.692\n",
      "Epoch 22, Batch 500, Loss: 0.694\n",
      "Epoch 22, Batch 600, Loss: 0.717\n",
      "Epoch 22, Batch 700, Loss: 0.710\n",
      "Epoch 23, Batch 100, Loss: 0.636\n",
      "Epoch 23, Batch 200, Loss: 0.655\n",
      "Epoch 23, Batch 300, Loss: 0.685\n",
      "Epoch 23, Batch 400, Loss: 0.694\n",
      "Epoch 23, Batch 500, Loss: 0.702\n",
      "Epoch 23, Batch 600, Loss: 0.695\n",
      "Epoch 23, Batch 700, Loss: 0.701\n",
      "Epoch 24, Batch 100, Loss: 0.625\n",
      "Epoch 24, Batch 200, Loss: 0.634\n",
      "Epoch 24, Batch 300, Loss: 0.667\n",
      "Epoch 24, Batch 400, Loss: 0.676\n",
      "Epoch 24, Batch 500, Loss: 0.692\n",
      "Epoch 24, Batch 600, Loss: 0.685\n",
      "Epoch 24, Batch 700, Loss: 0.691\n",
      "Epoch 25, Batch 100, Loss: 0.618\n",
      "Epoch 25, Batch 200, Loss: 0.598\n",
      "Epoch 25, Batch 300, Loss: 0.651\n",
      "Epoch 25, Batch 400, Loss: 0.645\n",
      "Epoch 25, Batch 500, Loss: 0.654\n",
      "Epoch 25, Batch 600, Loss: 0.649\n",
      "Epoch 25, Batch 700, Loss: 0.675\n",
      "Epoch 26, Batch 100, Loss: 0.602\n",
      "Epoch 26, Batch 200, Loss: 0.618\n",
      "Epoch 26, Batch 300, Loss: 0.627\n",
      "Epoch 26, Batch 400, Loss: 0.660\n",
      "Epoch 26, Batch 500, Loss: 0.642\n",
      "Epoch 26, Batch 600, Loss: 0.653\n",
      "Epoch 26, Batch 700, Loss: 0.666\n",
      "Epoch 27, Batch 100, Loss: 0.553\n",
      "Epoch 27, Batch 200, Loss: 0.597\n",
      "Epoch 27, Batch 300, Loss: 0.603\n",
      "Epoch 27, Batch 400, Loss: 0.638\n",
      "Epoch 27, Batch 500, Loss: 0.647\n",
      "Epoch 27, Batch 600, Loss: 0.648\n",
      "Epoch 27, Batch 700, Loss: 0.660\n",
      "Epoch 28, Batch 100, Loss: 0.561\n",
      "Epoch 28, Batch 200, Loss: 0.573\n",
      "Epoch 28, Batch 300, Loss: 0.575\n",
      "Epoch 28, Batch 400, Loss: 0.605\n",
      "Epoch 28, Batch 500, Loss: 0.642\n",
      "Epoch 28, Batch 600, Loss: 0.643\n",
      "Epoch 28, Batch 700, Loss: 0.633\n",
      "Epoch 29, Batch 100, Loss: 0.537\n",
      "Epoch 29, Batch 200, Loss: 0.568\n",
      "Epoch 29, Batch 300, Loss: 0.573\n",
      "Epoch 29, Batch 400, Loss: 0.591\n",
      "Epoch 29, Batch 500, Loss: 0.603\n",
      "Epoch 29, Batch 600, Loss: 0.633\n",
      "Epoch 29, Batch 700, Loss: 0.624\n",
      "Epoch 30, Batch 100, Loss: 0.530\n",
      "Epoch 30, Batch 200, Loss: 0.522\n",
      "Epoch 30, Batch 300, Loss: 0.581\n",
      "Epoch 30, Batch 400, Loss: 0.575\n",
      "Epoch 30, Batch 500, Loss: 0.625\n",
      "Epoch 30, Batch 600, Loss: 0.621\n",
      "Epoch 30, Batch 700, Loss: 0.634\n",
      "Epoch 31, Batch 100, Loss: 0.548\n",
      "Epoch 31, Batch 200, Loss: 0.531\n",
      "Epoch 31, Batch 300, Loss: 0.552\n",
      "Epoch 31, Batch 400, Loss: 0.543\n",
      "Epoch 31, Batch 500, Loss: 0.590\n",
      "Epoch 31, Batch 600, Loss: 0.614\n",
      "Epoch 31, Batch 700, Loss: 0.601\n",
      "Epoch 32, Batch 100, Loss: 0.523\n",
      "Epoch 32, Batch 200, Loss: 0.529\n",
      "Epoch 32, Batch 300, Loss: 0.547\n",
      "Epoch 32, Batch 400, Loss: 0.560\n",
      "Epoch 32, Batch 500, Loss: 0.584\n",
      "Epoch 32, Batch 600, Loss: 0.589\n",
      "Epoch 32, Batch 700, Loss: 0.601\n",
      "Epoch 33, Batch 100, Loss: 0.485\n",
      "Epoch 33, Batch 200, Loss: 0.521\n",
      "Epoch 33, Batch 300, Loss: 0.567\n",
      "Epoch 33, Batch 400, Loss: 0.562\n",
      "Epoch 33, Batch 500, Loss: 0.582\n",
      "Epoch 33, Batch 600, Loss: 0.590\n",
      "Epoch 33, Batch 700, Loss: 0.583\n",
      "Epoch 34, Batch 100, Loss: 0.503\n",
      "Epoch 34, Batch 200, Loss: 0.483\n",
      "Epoch 34, Batch 300, Loss: 0.501\n",
      "Epoch 34, Batch 400, Loss: 0.546\n",
      "Epoch 34, Batch 500, Loss: 0.554\n",
      "Epoch 34, Batch 600, Loss: 0.569\n",
      "Epoch 34, Batch 700, Loss: 0.595\n",
      "Epoch 35, Batch 100, Loss: 0.483\n",
      "Epoch 35, Batch 200, Loss: 0.515\n",
      "Epoch 35, Batch 300, Loss: 0.509\n",
      "Epoch 35, Batch 400, Loss: 0.528\n",
      "Epoch 35, Batch 500, Loss: 0.545\n",
      "Epoch 35, Batch 600, Loss: 0.567\n",
      "Epoch 35, Batch 700, Loss: 0.564\n",
      "Epoch 36, Batch 100, Loss: 0.477\n",
      "Epoch 36, Batch 200, Loss: 0.497\n",
      "Epoch 36, Batch 300, Loss: 0.511\n",
      "Epoch 36, Batch 400, Loss: 0.522\n",
      "Epoch 36, Batch 500, Loss: 0.553\n",
      "Epoch 36, Batch 600, Loss: 0.541\n",
      "Epoch 36, Batch 700, Loss: 0.572\n",
      "Epoch 37, Batch 100, Loss: 0.480\n",
      "Epoch 37, Batch 200, Loss: 0.485\n",
      "Epoch 37, Batch 300, Loss: 0.485\n",
      "Epoch 37, Batch 400, Loss: 0.513\n",
      "Epoch 37, Batch 500, Loss: 0.537\n",
      "Epoch 37, Batch 600, Loss: 0.562\n",
      "Epoch 37, Batch 700, Loss: 0.555\n",
      "Epoch 38, Batch 100, Loss: 0.447\n",
      "Epoch 38, Batch 200, Loss: 0.472\n",
      "Epoch 38, Batch 300, Loss: 0.492\n",
      "Epoch 38, Batch 400, Loss: 0.490\n",
      "Epoch 38, Batch 500, Loss: 0.511\n",
      "Epoch 38, Batch 600, Loss: 0.540\n",
      "Epoch 38, Batch 700, Loss: 0.528\n",
      "Epoch 39, Batch 100, Loss: 0.461\n",
      "Epoch 39, Batch 200, Loss: 0.463\n",
      "Epoch 39, Batch 300, Loss: 0.476\n",
      "Epoch 39, Batch 400, Loss: 0.515\n",
      "Epoch 39, Batch 500, Loss: 0.523\n",
      "Epoch 39, Batch 600, Loss: 0.510\n",
      "Epoch 39, Batch 700, Loss: 0.539\n",
      "Epoch 40, Batch 100, Loss: 0.438\n",
      "Epoch 40, Batch 200, Loss: 0.463\n",
      "Epoch 40, Batch 300, Loss: 0.470\n",
      "Epoch 40, Batch 400, Loss: 0.511\n",
      "Epoch 40, Batch 500, Loss: 0.518\n",
      "Epoch 40, Batch 600, Loss: 0.525\n",
      "Epoch 40, Batch 700, Loss: 0.522\n",
      "Epoch 41, Batch 100, Loss: 0.444\n",
      "Epoch 41, Batch 200, Loss: 0.448\n",
      "Epoch 41, Batch 300, Loss: 0.466\n",
      "Epoch 41, Batch 400, Loss: 0.491\n",
      "Epoch 41, Batch 500, Loss: 0.500\n",
      "Epoch 41, Batch 600, Loss: 0.529\n",
      "Epoch 41, Batch 700, Loss: 0.543\n",
      "Epoch 42, Batch 100, Loss: 0.448\n",
      "Epoch 42, Batch 200, Loss: 0.434\n",
      "Epoch 42, Batch 300, Loss: 0.484\n",
      "Epoch 42, Batch 400, Loss: 0.493\n",
      "Epoch 42, Batch 500, Loss: 0.484\n",
      "Epoch 42, Batch 600, Loss: 0.498\n",
      "Epoch 42, Batch 700, Loss: 0.525\n",
      "Epoch 43, Batch 100, Loss: 0.427\n",
      "Epoch 43, Batch 200, Loss: 0.435\n",
      "Epoch 43, Batch 300, Loss: 0.447\n",
      "Epoch 43, Batch 400, Loss: 0.454\n",
      "Epoch 43, Batch 500, Loss: 0.484\n",
      "Epoch 43, Batch 600, Loss: 0.508\n",
      "Epoch 43, Batch 700, Loss: 0.524\n",
      "Epoch 44, Batch 100, Loss: 0.432\n",
      "Epoch 44, Batch 200, Loss: 0.419\n",
      "Epoch 44, Batch 300, Loss: 0.435\n",
      "Epoch 44, Batch 400, Loss: 0.470\n",
      "Epoch 44, Batch 500, Loss: 0.481\n",
      "Epoch 44, Batch 600, Loss: 0.507\n",
      "Epoch 44, Batch 700, Loss: 0.519\n",
      "Epoch 45, Batch 100, Loss: 0.423\n",
      "Epoch 45, Batch 200, Loss: 0.431\n",
      "Epoch 45, Batch 300, Loss: 0.447\n",
      "Epoch 45, Batch 400, Loss: 0.444\n",
      "Epoch 45, Batch 500, Loss: 0.471\n",
      "Epoch 45, Batch 600, Loss: 0.492\n",
      "Epoch 45, Batch 700, Loss: 0.503\n",
      "Epoch 46, Batch 100, Loss: 0.412\n",
      "Epoch 46, Batch 200, Loss: 0.415\n",
      "Epoch 46, Batch 300, Loss: 0.416\n",
      "Epoch 46, Batch 400, Loss: 0.468\n",
      "Epoch 46, Batch 500, Loss: 0.487\n",
      "Epoch 46, Batch 600, Loss: 0.501\n",
      "Epoch 46, Batch 700, Loss: 0.489\n",
      "Epoch 47, Batch 100, Loss: 0.414\n",
      "Epoch 47, Batch 200, Loss: 0.410\n",
      "Epoch 47, Batch 300, Loss: 0.464\n",
      "Epoch 47, Batch 400, Loss: 0.461\n",
      "Epoch 47, Batch 500, Loss: 0.460\n",
      "Epoch 47, Batch 600, Loss: 0.465\n",
      "Epoch 47, Batch 700, Loss: 0.506\n",
      "Epoch 48, Batch 100, Loss: 0.409\n",
      "Epoch 48, Batch 200, Loss: 0.405\n",
      "Epoch 48, Batch 300, Loss: 0.434\n",
      "Epoch 48, Batch 400, Loss: 0.430\n",
      "Epoch 48, Batch 500, Loss: 0.440\n",
      "Epoch 48, Batch 600, Loss: 0.464\n",
      "Epoch 48, Batch 700, Loss: 0.479\n",
      "Epoch 49, Batch 100, Loss: 0.394\n",
      "Epoch 49, Batch 200, Loss: 0.424\n",
      "Epoch 49, Batch 300, Loss: 0.431\n",
      "Epoch 49, Batch 400, Loss: 0.439\n",
      "Epoch 49, Batch 500, Loss: 0.467\n",
      "Epoch 49, Batch 600, Loss: 0.466\n",
      "Epoch 49, Batch 700, Loss: 0.464\n",
      "Epoch 50, Batch 100, Loss: 0.402\n",
      "Epoch 50, Batch 200, Loss: 0.392\n",
      "Epoch 50, Batch 300, Loss: 0.414\n",
      "Epoch 50, Batch 400, Loss: 0.435\n",
      "Epoch 50, Batch 500, Loss: 0.437\n",
      "Epoch 50, Batch 600, Loss: 0.433\n",
      "Epoch 50, Batch 700, Loss: 0.482\n",
      "Accuracy on the 10000 test images: 64.26%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# CIFAR-10 dataset loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Instantiate model and move to GPU\n",
    "model = VisionTransformer().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):  # Adjust the number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Testing loop\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        # Move data to GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the 10000 test images: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
