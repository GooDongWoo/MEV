{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [4]\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "DISTRIBUTED_STRATEGY = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.NcclAllReduce(),\n",
    "    devices=['/gpu:%d' % index for index in range(len(SELECTED_GPUS))]\n",
    ")\n",
    "\n",
    "NUM_GPUS = DISTRIBUTED_STRATEGY.num_replicas_in_sync\n",
    "\n",
    "print('Number of devices: {}'.format(NUM_GPUS))\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from skimage import transform\n",
    "from vit_keras import vit\n",
    "from vit_keras.layers import ClassToken, AddPositionEmbs, MultiHeadSelfAttention, TransformerBlock\n",
    "\n",
    "PRECOMPUTE_DIR = 'precompute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch_id(branch_number):\n",
    "    if branch_number == 1:\n",
    "        return 'transformer_block'\n",
    "    else:\n",
    "        return 'transformer_block_%d' % (branch_number - 1)\n",
    "\n",
    "def get_model(dataset):\n",
    "    backbone_model = tf.keras.models.load_model('vit_%s_v1.h5' % dataset, custom_objects={\n",
    "        'ClassToken': ClassToken,\n",
    "        'AddPositionEmbs': AddPositionEmbs,\n",
    "        'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "        'TransformerBlock': TransformerBlock,\n",
    "    })\n",
    "\n",
    "    # freeze\n",
    "    for layer in backbone_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    outputs = []\n",
    "    for branch_number in range(1, 12):\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        outputs.append(y)\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=backbone_model.get_layer(index=0).input,\n",
    "        outputs=outputs\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute(dataset, batch_size=32 * NUM_GPUS):\n",
    "    with DISTRIBUTED_STRATEGY.scope():\n",
    "        model = get_model(dataset)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(split)\n",
    "        total_count = sum([1 if file_name.startswith(split) else 0 for file_name in os.listdir(dataset)])\n",
    "        batch_count = math.ceil(total_count / batch_size)\n",
    "        for batch_index in range(batch_count):\n",
    "            sys.stdout.write('\\r[%d/%d]' % (batch_index + 1, batch_count))\n",
    "            sys.stdout.flush()\n",
    "            images = []\n",
    "            labels = []\n",
    "            for sample_index in range(batch_index * batch_size, (batch_index + 1) * batch_size):\n",
    "                image_path = os.path.join(dataset, '%s_%d.pkl' % (split, sample_index))\n",
    "                if os.path.exists(image_path):  # last batch may contain less\n",
    "                    with open(image_path, 'rb') as cache_file:\n",
    "                        contents = pickle.load(cache_file)\n",
    "                        images.append(contents['image'])\n",
    "                        labels.append(contents['label'])\n",
    "            outputs = model(np.array(images))\n",
    "            for branch_number in range(1, 12):\n",
    "                branch_outputs = outputs[branch_number - 1]\n",
    "                for i, branch_output in enumerate(branch_outputs):\n",
    "                    sample_index = batch_index * batch_size + i\n",
    "                    sample_path = os.path.join(\n",
    "                        PRECOMPUTE_DIR,\n",
    "                        dataset,\n",
    "                        '%s_branch%d_sample%d.pkl' % (split, branch_number, sample_index)\n",
    "                    )\n",
    "                    with open(sample_path, 'wb') as sample_file:\n",
    "                        pickle.dump({\n",
    "                            'features': branch_output,\n",
    "                            'label': labels[i],\n",
    "                        }, sample_file)\n",
    "        print()  # newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute('cifar10')\n",
    "precompute('cifar100')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
