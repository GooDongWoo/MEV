{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from skimage import transform\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "from vit_keras import vit\n",
    "from vit_keras.layers import ClassToken, AddPositionEmbs, MultiHeadSelfAttention, TransformerBlock\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "HIDDEN_DIM = 768\n",
    "PATCH_SIZE = 16\n",
    "MLP_DIM = 3072  # ResMLP\n",
    "CHANNELS_MLP_DIM = 3072  # MLP-Mixer\n",
    "TOKENS_MLP_DIM = 384  # MLP-Mixer\n",
    "VIDEO_PATCHES = (2, 3)  # how many sub-images there are in each image for crowd counting\n",
    "VIDEO_SIZE = (VIDEO_PATCHES[0] * IMAGE_SIZE, VIDEO_PATCHES[1] * IMAGE_SIZE)\n",
    "\n",
    "def get_params(model):\n",
    "    string_list = []\n",
    "    model.summary(print_fn=lambda x: string_list.append(x))\n",
    "    for string in string_list:\n",
    "        if string.startswith('Trainable params:'):\n",
    "            return int(string.split()[-1].replace(',', ''))\n",
    "    return None\n",
    "\n",
    "def get_flops(model):\n",
    "    \"\"\"\n",
    "    from https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-768977280\n",
    "    \"\"\"\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops\n",
    "\n",
    "def get_branch_id(branch_number):\n",
    "    if branch_number == 1:\n",
    "        return 'transformer_block'\n",
    "    else:\n",
    "        return 'transformer_block_%d' % (branch_number - 1)\n",
    "\n",
    "def get_model(branch_numbers, head_type, dataset):\n",
    "    model_file_name = 'vit_cifar10_v1.h5'\n",
    "    backbone_model = tf.keras.models.load_model(model_file_name, custom_objects={\n",
    "        'ClassToken': ClassToken,\n",
    "        'AddPositionEmbs': AddPositionEmbs,\n",
    "        'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "        'TransformerBlock': TransformerBlock,\n",
    "    })\n",
    "\n",
    "    outputs = []\n",
    "    for i, branch_number in enumerate(branch_numbers):\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        y, _ = TransformerBlock(\n",
    "            num_heads=12,\n",
    "            mlp_dim=3072,\n",
    "            dropout=0.1,\n",
    "            name='Transformer/encoderblock_x_%d' % i\n",
    "        )(y)\n",
    "        y = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6,\n",
    "            name='Transformer/encoder_norm_x_%d' % i\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x_%d' % i)(y)\n",
    "\n",
    "        output_units = 10\n",
    "        output_activation = 'softmax'\n",
    "\n",
    "        # MLP head\n",
    "        initializer = tf.keras.initializers.he_normal()\n",
    "        regularizer = tf.keras.regularizers.l2()\n",
    "        y = tf.keras.layers.Dense(\n",
    "            units=256,\n",
    "            activation='elu',\n",
    "            kernel_initializer=initializer,\n",
    "            kernel_regularizer=regularizer\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Dropout(0.5)(y)\n",
    "        y = tf.keras.layers.Dense(\n",
    "            units=256,\n",
    "            activation='elu',\n",
    "            kernel_initializer=initializer,\n",
    "            kernel_regularizer=regularizer\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Dropout(0.5)(y)\n",
    "        y = tf.keras.layers.Dense(\n",
    "            units=output_units,\n",
    "            activation=output_activation,\n",
    "            kernel_initializer=initializer,\n",
    "            kernel_regularizer=regularizer\n",
    "        )(y)\n",
    "        outputs.append(y)\n",
    "\n",
    "    outputs.append(backbone_model.get_layer(index=-1).output)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=backbone_model.get_layer(index=0).input,\n",
    "        outputs=outputs\n",
    "    )\n",
    "\n",
    "    loss_type = 'categorical_crossentropy'\n",
    "    metric_type = 'accuracy'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=[loss_type] * (len(branch_numbers) + 1),\n",
    "        loss_weights=[1] * len(branch_numbers) + [2],\n",
    "        metrics=[metric_type]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def cache_split(cache_dir, images, labels, split):\n",
    "    for i in range(images.shape[0]):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            sys.stdout.write('\\r%d' % (i + 1))\n",
    "            sys.stdout.flush()\n",
    "        with open(os.path.join(cache_dir, '%s_%d.pkl' % (split, i)), 'wb') as cache_file:\n",
    "            pickle.dump({\n",
    "                'image': transform.resize(images[i], (IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                'label': labels[i],\n",
    "            }, cache_file)\n",
    "    print()  # newline\n",
    "\n",
    "def cache_all(dataset):\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "    val_index = int(len(train_images) * 0.8)\n",
    "    val_images = train_images[val_index:]\n",
    "    val_labels = train_labels[val_index:]\n",
    "    train_images = train_images[:val_index]\n",
    "    train_labels = train_labels[:val_index]\n",
    "\n",
    "    cache_split(dataset, train_images, train_labels, 'train')\n",
    "    cache_split(dataset, val_images, val_labels, 'val')\n",
    "    cache_split(dataset, test_images, test_labels, 'test')\n",
    "\n",
    "class CIFARSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, split, batch_size, dataset):\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.cache_dir = dataset\n",
    "        self.count = sum([1 if file_name.startswith(split) else 0 for file_name in os.listdir(self.cache_dir)])\n",
    "        self.random_permutation = np.random.permutation(self.count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.count / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.random_permutation = np.random.permutation(self.count)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for i in self.random_permutation[index * self.batch_size:(index + 1) * self.batch_size]:\n",
    "            with open(os.path.join(self.cache_dir, '%s_%d.pkl' % (self.split, i)), 'rb') as cache_file:\n",
    "                contents = pickle.load(cache_file)\n",
    "                images.append(contents['image'])\n",
    "                labels.append(contents['label'])\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "def train(max_epochs, branch_numbers, head_type, dataset, version, temporary):\n",
    "    model = get_model(branch_numbers, head_type, dataset)\n",
    "\n",
    "    lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    save_model_checkpoint_file = 'bmvc_rebuttal_ee_v%d_%s_%s_%s.h5' % (\n",
    "        version,\n",
    "        head_type,\n",
    "        dataset,\n",
    "        '-'.join([str(branch_number) for branch_number in branch_numbers])\n",
    "    )\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        save_model_checkpoint_file,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "\n",
    "    callbacks = [lr_reduce, early_stop]\n",
    "    if not temporary:\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    batch_size = 4\n",
    "    train_sequence = CIFARSequence('train', batch_size, dataset)\n",
    "    val_sequence = CIFARSequence('val', batch_size, dataset)\n",
    "    test_sequence = CIFARSequence('test', batch_size, dataset)\n",
    "    \n",
    "\n",
    "    history = model.fit(\n",
    "        train_sequence,\n",
    "        validation_data=val_sequence,\n",
    "        epochs=max_epochs,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_accuracy = model.evaluate(test_sequence)[1]\n",
    "\n",
    "    model_params = get_params(model) / 10 ** 6\n",
    "    model_flops = get_flops(model) / 10 ** 9\n",
    "\n",
    "    return model, test_accuracy, model_params, model_flops\n",
    "\n",
    "cache_all('cifar10')\n",
    "cache_all('cifar100')\n",
    "model, test_accuracy, model_params, model_flops = train(\n",
    "    max_epochs=100,\n",
    "    branch_numbers=[3, 6, 9],\n",
    "    head_type='resmlp',\n",
    "    dataset='disco',\n",
    "    version=5,\n",
    "    temporary=False\n",
    ")\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
