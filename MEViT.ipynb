{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms,models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # Importing tqdm for progress bar\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "from torchsummary import summary as summary_\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Check if GPU is available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# 데이터셋 전처리 설정: CIFAR-10을 사용하고 이미지 크기를 32로 리사이즈\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# Load the pretrained ViT model from the saved file\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pretrained_vit = models.vit_b_16(weights=None)\n",
    "pretrained_vit.heads.head = nn.Linear(pretrained_vit.heads.head.in_features, 10)  # Ensure output matches the number of classes\n",
    "\n",
    "# Load model weights\n",
    "pretrained_vit.load_state_dict(torch.load('vit_cifar10_v1.pth', map_location=device))\n",
    "pretrained_vit = pretrained_vit.to(device)\n",
    "#from torchinfo import summary\n",
    "#summary(pretrained_vit,input_size= (64, 3, IMG_SIZE, IMG_SIZE))\n",
    "#a = nn.ModuleList([pretrained_vit.create_exit_Tblock(dim, num_classes) for _ in range(len(ee_list))])\n",
    "b= nn.ModuleList([nn.Linear(3, 5) for _ in range(5)])\n",
    "b[4]\n",
    "class MultiExitViT(nn.Module):\n",
    "    def __init__(self, base_model,dim=768, ee_list=[0,1,2,3,4,5,6,7,8,9],exit_loss_weights=[1,1,1,1,1,1,1,1,1,1],num_classes=10,image_size=IMG_SIZE,patch_size=16):\n",
    "        super(MultiExitViT, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.patch_size=patch_size\n",
    "        self.hidden_dim=dim\n",
    "        self.image_size=image_size\n",
    "        \n",
    "        self.exit_loss_weights = [elw/sum(exit_loss_weights) for elw in exit_loss_weights]\n",
    "        \n",
    "        # Multiple Exit Blocks 추가\n",
    "        self.num_exits = len(ee_list)+1\n",
    "        self.ee_list = ee_list\n",
    "        self.ees = nn.ModuleList([self.create_exit_Tblock(dim) for _ in range(len(ee_list))])\n",
    "        self.classifiers = nn.ModuleList([nn.Linear(dim, num_classes) for _ in range(len(ee_list))])\n",
    "        \n",
    "        # base model load\n",
    "        self.conv_proj = self.base_model.conv_proj\n",
    "        self.encoder_blocks = nn.ModuleList([encoderblock for encoderblock in [*pretrained_vit.encoder.layers]])\n",
    "        \n",
    "        # Final head\n",
    "        self.ln= self.base_model.encoder.ln\n",
    "        self.heads = self.base_model.heads\n",
    "\n",
    "    def create_exit_Tblock(self, dim):\n",
    "        return nn.Sequential(\n",
    "            models.vision_transformer.EncoderBlock(num_heads=12, hidden_dim=dim, mlp_dim= 3072, dropout=0.0, attention_dropout=0.0),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "        \n",
    "    def getELW(self):\n",
    "        if(self.exit_loss_weights is None):\n",
    "            self.exit_loss_weights = [1]*self.num_exits\n",
    "        return self.exit_loss_weights\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        #x = self.encoder(x)\n",
    "        for idx, block in enumerate(self.encoder_blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.ee_list:\n",
    "                y = self.ees[idx](x)\n",
    "                y = y[:, 0]\n",
    "                y = self.classifiers[idx](y)\n",
    "                outputs.append(y)\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        # Append the final output from the original head\n",
    "        x = self.ln(x)\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x = self.heads(x)\n",
    "        outputs.append(x)\n",
    "        return outputs\n",
    "# # 3. Training part\n",
    "# function to get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# function to calculate metric per mini-batch\n",
    "def metric_batch(output, label):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(label.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "# function to calculate loss per mini-batch\n",
    "def loss_batch(loss_func, output_list, label, elws, opt=None):\n",
    "    losses = [loss_func(output,label)*elw for output,elw in zip(output_list,elws)]   # raw losses -> 굳이 각각 exit의 길이로 나눠줘야하나? 로스 크기만 달라지지 않나;;\n",
    "    acc_s = [metric_batch(output, label) for output in output_list]\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        #backprop\n",
    "        for loss in losses[:-1]:\n",
    "            #ee losses need to keep graph\n",
    "            loss.backward(retain_graph=True)\n",
    "        #final loss, graph not required\n",
    "        losses[-1].backward()\n",
    "        opt.step()\n",
    "    \n",
    "    losses = [loss.item() for loss in losses] #for out of cuda memory error\n",
    "    \n",
    "    return losses, acc_s\n",
    "\n",
    "# function to calculate loss and metric per epoch\n",
    "def loss_epoch(model, loss_func, dataset_dl, writer, epoch, opt=None):\n",
    "    device = next(model.parameters()).device\n",
    "    running_loss = [0.0] * model.exit_num\n",
    "    running_metric = [0.0] * model.exit_num\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "    TorV='train' if opt is not None else 'val'\n",
    "    for xb, yb in tqdm(dataset_dl, desc=TorV, leave=False):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output_list = model(xb)\n",
    "        elws=model.getELW()\n",
    "\n",
    "        losses, acc_s = loss_batch(loss_func, output_list, yb, elws, opt)\n",
    "\n",
    "        running_loss = [sum(i) for i in zip(running_loss,losses)]\n",
    "        running_metric = [sum(i) for i in zip(running_metric,acc_s)]\n",
    "    \n",
    "    running_loss=[i/len_data for i in running_loss]\n",
    "    running_acc=[100*i/len_data for i in running_metric]\n",
    "    \n",
    "    # Tensorboard\n",
    "    tmp_loss_dict = dict();tmp_acc_dict = dict()\n",
    "    for idx in range(model.exit_num):\n",
    "        tmp_loss_dict[f'exit{idx}'] = running_loss[idx];tmp_acc_dict[f'exit{idx}'] = running_acc[idx]\n",
    "    writer.add_scalars(f'{TorV}/loss', tmp_loss_dict, epoch)\n",
    "    writer.add_scalars(f'{TorV}/acc', tmp_acc_dict, epoch)\n",
    "    \n",
    "    losses_sum = sum(running_loss) # float\n",
    "    writer.add_scalar(f'{TorV}/loss_total_sum', losses_sum, epoch)\n",
    "    accs = running_acc # float list[exit_num]\n",
    "\n",
    "    return losses_sum, accs\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):   #TODO 모델 불러오기\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    isload=params[\"isload\"]\n",
    "    path_chckpnt=params[\"path_chckpnt\"]\n",
    "    resize=params[\"resize\"]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # path to save the model weights\n",
    "    current_time = time.strftime('%m%d_%H%M%S', time.localtime())\n",
    "    path=f'./models/{current_time}'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    spec_txt=f'opt: {opt.__class__.__name__}\\nlr: {opt.param_groups[0][\"lr\"]}\\nbatch: {train_dl.batch_size}\\nepoch: {num_epochs}\\nisload: {isload}\\npath_chckpnt: {path_chckpnt}\\nexits_loss_weights: {model.getELW()}\\n'\n",
    "    with open(f\"{path}/spec.txt\", \"w\") as file:\n",
    "        file.write(spec_txt)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    old_epoch=0\n",
    "    if(isload):\n",
    "        chckpnt = torch.load(path_chckpnt,weights_only=True)\n",
    "        model.load_state_dict(chckpnt['model_state_dict'])\n",
    "        opt.load_state_dict(chckpnt['optimizer_state_dict'])\n",
    "        old_epoch = chckpnt['epoch']\n",
    "        best_loss = chckpnt['loss']\n",
    "    \n",
    "    #writer=None\n",
    "    writer = SummaryWriter('./runs/'+current_time,)\n",
    "    writer.add_graph(model, torch.rand(1,3,resize,resize).to(next(model.parameters()).device))\n",
    "    \n",
    "    for epoch in range(old_epoch,old_epoch+num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, old_epoch+num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_accs = loss_epoch(model, loss_func, train_dl, writer, epoch, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_accs = loss_epoch(model, loss_func, val_dl, writer, epoch, opt=None)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path+'/best_model.pth')\n",
    "            print('saved best model weights!')\n",
    "            print('Get best val_loss')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        total_time=(time.time()-start_time)/60\n",
    "        hours, minutes = divmod(total_time, 60)\n",
    "        print(f'train_loss: {train_loss:.6f}, train_acc: {train_accs}')\n",
    "        print(f'val_loss: {val_loss:.6f}, val_acc: {val_accs}, time: {int(hours)}h {int(minutes)}m')\n",
    "        print('-'*10)\n",
    "        writer.flush()\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            }, path+'/chckpoint.pth')\n",
    "    writer.close()\n",
    "    linedivisor='#'*10+'\\n'\n",
    "    result_txt=linedivisor+f'last_val_acc: {val_accs}\\nlast_train_acc: {train_accs}\\nlast_val_loss: {best_loss:.6f}\\ntotal_time: {total_time:.2f}m\\n'\n",
    "    with open(f\"{path}/spec.txt\", \"a\") as file:\n",
    "        file.write(result_txt)    \n",
    "    \n",
    "    return model\n",
    "model = MultiExitViT(pretrained_vit).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler=ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=5, verbose=True)\n",
    "params={'num_epochs':5, 'loss_func':criterion, 'optimizer':optimizer, \n",
    "        'train_dl':train_loader, 'val_dl':val_loader, 'lr_scheduler':lr_scheduler, \n",
    "        'isload':True, 'path_chckpnt':'vit_cifar10_v1.pth', 'resize':IMG_SIZE}\n",
    "\n",
    "train_val(model=model, params=params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
