{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [4, 5, 6, 7]\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "DISTRIBUTED_STRATEGY = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.NcclAllReduce(),\n",
    "    devices=['/gpu:%d' % index for index in range(len(SELECTED_GPUS))]\n",
    ")\n",
    "\n",
    "NUM_GPUS = DISTRIBUTED_STRATEGY.num_replicas_in_sync\n",
    "\n",
    "print('Number of devices: {}'.format(NUM_GPUS))\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from skimage import transform\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "from vit_keras import vit\n",
    "from vit_keras.layers import ClassToken, AddPositionEmbs, MultiHeadSelfAttention, TransformerBlock\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "HIDDEN_DIM = 768\n",
    "PATCH_SIZE = 16\n",
    "MLP_DIM = 3072  # ResMLP\n",
    "CHANNELS_MLP_DIM = 3072  # MLP-Mixer\n",
    "TOKENS_MLP_DIM = 384  # MLP-Mixer\n",
    "VIDEO_PATCHES = (2, 3)  # how many sub-images there are in each image for crowd counting\n",
    "VIDEO_SIZE = (VIDEO_PATCHES[0] * IMAGE_SIZE, VIDEO_PATCHES[1] * IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model):\n",
    "    string_list = []\n",
    "    model.summary(print_fn=lambda x: string_list.append(x))\n",
    "    for string in string_list:\n",
    "        if string.startswith('Trainable params:'):\n",
    "            return int(string.split()[-1].replace(',', ''))\n",
    "    return None\n",
    "\n",
    "def get_flops(model):\n",
    "    \"\"\"\n",
    "    from https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-768977280\n",
    "    \"\"\"\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/leondgarse/Keras_mlp/blob/main/res_mlp.py\n",
    "\n",
    "def channel_affine(inputs, use_bias=True, weight_init_value=1, name=''):\n",
    "    ww_init = tfkeras.initializers.Constant(weight_init_value) if weight_init_value != 1 else 'ones'\n",
    "    nn = tf.keras.backend.expand_dims(inputs, 1)\n",
    "    nn = tf.keras.layers.DepthwiseConv2D(1, depthwise_initializer=ww_init, use_bias=use_bias, name=name + 'affine')(nn)\n",
    "    return tf.keras.backend.squeeze(nn, 1)\n",
    "\n",
    "def mlp_block(inputs, mlp_dim, activation='gelu', name=''):\n",
    "    affine_inputs = channel_affine(inputs, use_bias=True, name=name + '1_')\n",
    "    nn = tf.keras.layers.Permute((2, 1), name=name + 'permute_1')(affine_inputs)\n",
    "    nn = tf.keras.layers.Dense(nn.shape[-1], name=name + 'dense_1')(nn)\n",
    "    nn = tf.keras.layers.Permute((2, 1), name=name + 'permute_2')(nn)\n",
    "    nn = channel_affine(nn, use_bias=False, name=name + '1_gamma_')\n",
    "    skip_conn = tf.keras.layers.Add(name=name + 'add_1')([nn, affine_inputs])\n",
    "\n",
    "    affine_skip = channel_affine(skip_conn, use_bias=True, name=name + '2_')\n",
    "    nn = tf.keras.layers.Dense(mlp_dim, name=name + 'dense_2_1')(affine_skip)\n",
    "    nn = tf.keras.layers.Activation(activation, name=name + 'gelu')(nn)\n",
    "    nn = tf.keras.layers.Dense(inputs.shape[-1], name=name + 'dense_2_2')(nn)\n",
    "    nn = channel_affine(nn, use_bias=False, name=name + '2_gamma_')\n",
    "    nn = tf.keras.layers.Add(name=name + 'add_2')([nn, affine_skip])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/Benjamin-Etheredge/mlp-mixer-keras/blob/main/mlp_mixer_keras/mlp_mixer.py\n",
    "\n",
    "class MlpBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, activation=None, **kwargs):\n",
    "        super(MlpBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = tf.keras.activations.gelu\n",
    "\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        self.dense2 = tf.keras.layers.Dense(dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_signature):\n",
    "        return (input_signature[0], self.dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MlpBlock, self).get_config().copy()\n",
    "        config.update({\n",
    "            'dim': self.dim,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MixerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        channel_dim,\n",
    "        token_mixer_hidden_dim,\n",
    "        channel_mixer_hidden_dim=None,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MixerBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = tf.keras.activations.gelu\n",
    "\n",
    "        if channel_mixer_hidden_dim is None:\n",
    "            channel_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.channel_dim = channel_dim\n",
    "        self.token_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "        self.channel_mixer_hidden_dim = channel_mixer_hidden_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "        self.permute1 = tf.keras.layers.Permute((2, 1))\n",
    "        self.token_mixer = MlpBlock(num_patches, token_mixer_hidden_dim, name='token_mixer')\n",
    "\n",
    "        self.permute2 = tf.keras.layers.Permute((2, 1))\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "        self.channel_mixer = MlpBlock(channel_dim, channel_mixer_hidden_dim, name='channel_mixer')\n",
    "\n",
    "        self.skip_connection1 = tf.keras.layers.Add()\n",
    "        self.skip_connection2 = tf.keras.layers.Add()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MixerBlock, self).get_config().copy()\n",
    "        config.update({\n",
    "            'num_patches': self.num_patches,\n",
    "            'channel_dim': self.channel_dim,\n",
    "            'token_mixer_hidden_dim': self.token_mixer_hidden_dim,\n",
    "            'channel_mixer_hidden_dim': self.channel_mixer_hidden_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        skip_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.permute1(x)\n",
    "        x = self.token_mixer(x)\n",
    "\n",
    "        x = self.permute2(x)\n",
    "\n",
    "        x = self.skip_connection1([x, skip_x])\n",
    "        skip_x = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.channel_mixer(x)\n",
    "\n",
    "        x = self.skip_connection2([x, skip_x])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch_id(branch_number):\n",
    "    if branch_number == 1:\n",
    "        return 'transformer_block'\n",
    "    else:\n",
    "        return 'transformer_block_%d' % (branch_number - 1)\n",
    "\n",
    "def get_model(branch_numbers, head_type, dataset):\n",
    "    if dataset == 'cifar10':\n",
    "        model_file_name = 'vit_cifar10_v1.h5'\n",
    "    elif dataset == 'cifar100':\n",
    "        model_file_name = 'vit_cifar100_v1.h5'\n",
    "    elif dataset == 'disco':\n",
    "        model_file_name = 'vit_cc_backbone_v2.h5'\n",
    "    else:\n",
    "        model_file_name = None\n",
    "    \n",
    "    backbone_model = tf.keras.models.load_model(model_file_name, custom_objects={\n",
    "        'ClassToken': ClassToken,\n",
    "        'AddPositionEmbs': AddPositionEmbs,\n",
    "        'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
    "        'TransformerBlock': TransformerBlock,\n",
    "    })\n",
    "\n",
    "    outputs = []\n",
    "    for i, branch_number in enumerate(branch_numbers):\n",
    "        y, _ = backbone_model.get_layer(get_branch_id(branch_number)).output\n",
    "        if head_type == 'resmlp':\n",
    "            y = mlp_block(y, mlp_dim=MLP_DIM, name='mlp_mixer_%d' % i)\n",
    "            y = tf.keras.layers.GlobalAveragePooling1D()(y)\n",
    "        elif head_type == 'mlp':\n",
    "            y = tf.keras.layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name='Transformer/encoder_norm_x_%d' % i\n",
    "            )(y)\n",
    "            y = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x_%d' % i)(y)\n",
    "        elif head_type == 'vit':\n",
    "            y, _ = TransformerBlock(\n",
    "                num_heads=12,\n",
    "                mlp_dim=3072,\n",
    "                dropout=0.1,\n",
    "                name='Transformer/encoderblock_x_%d' % i\n",
    "            )(y)\n",
    "            y = tf.keras.layers.LayerNormalization(\n",
    "                epsilon=1e-6,\n",
    "                name='Transformer/encoder_norm_x_%d' % i\n",
    "            )(y)\n",
    "            y = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x_%d' % i)(y)\n",
    "        elif head_type == 'cnn_ignore':\n",
    "            channels = HIDDEN_DIM\n",
    "            width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "            y = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken_%d' % i)(y)\n",
    "            y = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape_%d' % i)(y)\n",
    "            y = tf.keras.layers.Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                activation='elu',\n",
    "                padding='same'\n",
    "            )(y)\n",
    "            y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "            y = tf.keras.layers.Flatten()(y)\n",
    "        elif head_type == 'cnn_add':    \n",
    "            channels = HIDDEN_DIM\n",
    "            width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "\n",
    "            y1 = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken_x_%d' % i)(y)\n",
    "            y1 = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape_%d' % i)(y1)\n",
    "\n",
    "            y2 = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x_%d' % i)(y)\n",
    "            y2 = tf.keras.layers.RepeatVector(width * height)(y2)\n",
    "            y2 = tf.keras.layers.Reshape((width, height, channels), name='cls_reshape_%d' % i)(y2)\n",
    "\n",
    "            y = tf.keras.layers.Add()([y1, y2])\n",
    "\n",
    "            y = tf.keras.layers.Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                activation='elu',\n",
    "                padding='same'\n",
    "            )(y)\n",
    "            y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "            y = tf.keras.layers.Flatten()(y)\n",
    "        elif head_type == 'cnn_project':\n",
    "            channels = HIDDEN_DIM\n",
    "            width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "\n",
    "            y1 = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken_x_%d' % i)(y)\n",
    "            y1 = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape_%d' % i)(y1)\n",
    "\n",
    "            y2 = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x_%d' % i)(y)\n",
    "            y2 = tf.keras.layers.RepeatVector(width * height)(y2)\n",
    "            y2 = tf.keras.layers.Reshape((width, height, channels), name='cls_reshape_%d' % i)(y2)\n",
    "\n",
    "            y = tf.keras.layers.Concatenate()([y1, y2])\n",
    "\n",
    "            y = tf.keras.layers.Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                activation='elu',\n",
    "                padding='same'\n",
    "            )(y)\n",
    "            y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "            y = tf.keras.layers.Flatten()(y)\n",
    "        elif head_type == 'mlp_mixer':\n",
    "            num_patches = (IMAGE_SIZE // PATCH_SIZE) ** 2 + 1\n",
    "            y = MixerBlock(\n",
    "                num_patches=num_patches,\n",
    "                channel_dim=HIDDEN_DIM,\n",
    "                token_mixer_hidden_dim=TOKENS_MLP_DIM,\n",
    "                channel_mixer_hidden_dim=CHANNELS_MLP_DIM\n",
    "            )(y)\n",
    "            y = tf.keras.layers.GlobalAveragePooling1D()(y)\n",
    "\n",
    "        if dataset == 'cifar10':\n",
    "            output_units = 10\n",
    "            output_activation = 'softmax'\n",
    "        elif dataset == 'cifar100':\n",
    "            output_units = 100\n",
    "            output_activation = 'softmax'\n",
    "        elif dataset == 'disco':\n",
    "            output_units = 1\n",
    "            output_activation = None\n",
    "        else:\n",
    "            output_units = None\n",
    "            output_activation = None\n",
    "\n",
    "        # MLP head\n",
    "        initializer = tf.keras.initializers.he_normal()\n",
    "        regularizer = tf.keras.regularizers.l2()\n",
    "        y = tf.keras.layers.Dense(\n",
    "            units=256,\n",
    "            activation='elu',\n",
    "            kernel_initializer=initializer,\n",
    "            kernel_regularizer=regularizer\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Dropout(0.5)(y)\n",
    "        y = tf.keras.layers.Dense(\n",
    "            units=256,\n",
    "            activation='elu',\n",
    "            kernel_initializer=initializer,\n",
    "            kernel_regularizer=regularizer\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Dropout(0.5)(y)\n",
    "        y = tf.keras.layers.Dense(\n",
    "            units=output_units,\n",
    "            activation=output_activation,\n",
    "            kernel_initializer=initializer,\n",
    "            kernel_regularizer=regularizer\n",
    "        )(y)\n",
    "        outputs.append(y)\n",
    "\n",
    "    outputs.append(backbone_model.get_layer(index=-1).output)\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=backbone_model.get_layer(index=0).input,\n",
    "        outputs=outputs\n",
    "    )\n",
    "\n",
    "    if dataset == 'cifar10' or dataset == 'cifar100':\n",
    "        loss_type = 'categorical_crossentropy'\n",
    "        metric_type = 'accuracy'\n",
    "    elif dataset == 'disco':\n",
    "        loss_type = 'mean_absolute_error'\n",
    "        metric_type = 'mean_absolute_error'\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=[loss_type] * (len(branch_numbers) + 1),\n",
    "        loss_weights=[1] * len(branch_numbers) + [2],\n",
    "        metrics=[metric_type]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_split(cache_dir, images, labels, split):\n",
    "    for i in range(images.shape[0]):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            sys.stdout.write('\\r%d' % (i + 1))\n",
    "            sys.stdout.flush()\n",
    "        with open(os.path.join(cache_dir, '%s_%d.pkl' % (split, i)), 'wb') as cache_file:\n",
    "            pickle.dump({\n",
    "                'image': transform.resize(images[i], (IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                'label': labels[i],\n",
    "            }, cache_file)\n",
    "    print()  # newline\n",
    "\n",
    "def cache_all(dataset):\n",
    "    if dataset == 'cifar10':\n",
    "        (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "    elif dataset == 'cifar100':\n",
    "        (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise Exception('Unknown dataset: %s' % dataset)\n",
    "\n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "    val_index = int(len(train_images) * 0.8)\n",
    "    val_images = train_images[val_index:]\n",
    "    val_labels = train_labels[val_index:]\n",
    "    train_images = train_images[:val_index]\n",
    "    train_labels = train_labels[:val_index]\n",
    "\n",
    "    cache_split(dataset, train_images, train_labels, 'train')\n",
    "    cache_split(dataset, val_images, val_labels, 'val')\n",
    "    cache_split(dataset, test_images, test_labels, 'test')\n",
    "\n",
    "class CIFARSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, split, batch_size, dataset):\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size * NUM_GPUS\n",
    "        self.cache_dir = dataset\n",
    "        self.count = sum([1 if file_name.startswith(split) else 0 for file_name in os.listdir(self.cache_dir)])\n",
    "        self.random_permutation = np.random.permutation(self.count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.count / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.random_permutation = np.random.permutation(self.count)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for i in self.random_permutation[index * self.batch_size:(index + 1) * self.batch_size]:\n",
    "            with open(os.path.join(self.cache_dir, '%s_%d.pkl' % (self.split, i)), 'rb') as cache_file:\n",
    "                contents = pickle.load(cache_file)\n",
    "                images.append(contents['image'])\n",
    "                labels.append(contents['label'])\n",
    "        return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_flip(image):\n",
    "    return np.flip(image, axis=1)\n",
    "\n",
    "class DISCOSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, split, batch_size):\n",
    "        self.split = split\n",
    "        self.cache_dir = os.path.join('disco', 'vit_cache')\n",
    "        self.split_len = sum([\n",
    "            1 if file_name.startswith(self.split) else 0 for file_name in os.listdir(self.cache_dir)\n",
    "        ])\n",
    "        self.batch_size = batch_size * NUM_GPUS\n",
    "        self.random_permutation = np.random.permutation(self.split_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.split_len / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.random_permutation = np.random.permutation(self.split_len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        spectrograms = []\n",
    "        images = []\n",
    "        density_maps = []\n",
    "        if self.split == 'test':\n",
    "            index_generator = range(\n",
    "                index * self.batch_size,\n",
    "                min((index + 1) * self.batch_size, self.split_len - 1)\n",
    "            )\n",
    "        else:\n",
    "            index_generator = self.random_permutation[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        for random_index in index_generator:\n",
    "            all_path = os.path.join(\n",
    "                self.cache_dir,\n",
    "                '%s_%d.pkl' % (self.split, random_index)\n",
    "            )\n",
    "            with open(all_path, 'rb') as all_file:\n",
    "                data = pickle.load(all_file)\n",
    "                if self.split == 'train' and random.random() < 0.5:  # flip augmentation\n",
    "                    images.append(horizontal_flip(data['image']))\n",
    "                else:\n",
    "                    images.append(data['image'])\n",
    "                density_maps.append(np.sum(data['density_map']))\n",
    "\n",
    "        return np.array(images), np.array(density_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cc(model, test_sequence, total_branches):\n",
    "    gt = None\n",
    "    outs = []\n",
    "    for i, (images, density_maps) in enumerate(test_sequence):\n",
    "        sys.stdout.write('\\r%d' % (i + 1))\n",
    "        sys.stdout.flush()\n",
    "        if gt is not None:\n",
    "            gt = np.concatenate((gt, density_maps))\n",
    "        else:\n",
    "            gt = density_maps\n",
    "        output = model(images)\n",
    "        for j in range(total_branches):\n",
    "            if i == 0:\n",
    "                outs.append(output[j].numpy().flatten())\n",
    "            else:\n",
    "                outs[j] = np.concatenate((outs[j], output[j].numpy().flatten()))\n",
    "    print()  # newline\n",
    "    maes = []\n",
    "    img_patches = VIDEO_PATCHES[0] * VIDEO_PATCHES[1]\n",
    "    for i in range(0, gt.shape[0], img_patches):\n",
    "        gt_subset = gt[i:i + img_patches]\n",
    "        for j in range(total_branches):\n",
    "            if i == 0:\n",
    "                maes.append([np.abs(np.sum(gt_subset) - np.sum(outs[j][i:i + img_patches]))])\n",
    "            else:\n",
    "                maes[j].append(np.abs(np.sum(gt_subset) - np.sum(outs[j][i:i + img_patches])))\n",
    "    return [np.mean(np.array(item)) for item in maes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epochs, branch_numbers, head_type, dataset, version, temporary):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    with DISTRIBUTED_STRATEGY.scope():\n",
    "        model = get_model(branch_numbers, head_type, dataset)\n",
    "\n",
    "    lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    save_model_checkpoint_file = 'bmvc_rebuttal_ee_v%d_%s_%s_%s.h5' % (\n",
    "        version,\n",
    "        head_type,\n",
    "        dataset,\n",
    "        '-'.join([str(branch_number) for branch_number in branch_numbers])\n",
    "    )\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        save_model_checkpoint_file,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "\n",
    "    callbacks = [lr_reduce, early_stop]\n",
    "    if not temporary:\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    batch_size = 4\n",
    "    if dataset == 'cifar10' or dataset == 'cifar100':\n",
    "        train_sequence = CIFARSequence('train', batch_size, dataset)\n",
    "        val_sequence = CIFARSequence('val', batch_size, dataset)\n",
    "        test_sequence = CIFARSequence('test', batch_size, dataset)\n",
    "    elif dataset == 'disco':\n",
    "        train_sequence = DISCOSequence('train', batch_size)\n",
    "        val_sequence = DISCOSequence('val', batch_size)\n",
    "        test_sequence = DISCOSequence('test', 2 * batch_size)\n",
    "    else:\n",
    "        raise Exception('Unknown dataset: %s' % dataset)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_sequence,\n",
    "        validation_data=val_sequence,\n",
    "        epochs=max_epochs,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    if dataset == 'cifar10' or dataset == 'cifar100':\n",
    "        test_accuracy = model.evaluate(test_sequence)[1]\n",
    "    elif dataset == 'disco':\n",
    "        test_accuracy = test_cc(model, test_sequence, len(branch_numbers) + 1)\n",
    "\n",
    "    model_params = get_params(model) / 10 ** 6\n",
    "    model_flops = get_flops(model) / 10 ** 9\n",
    "\n",
    "    return model, test_accuracy, model_params, model_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_all('cifar10')\n",
    "cache_all('cifar100')\n",
    "model, test_accuracy, model_params, model_flops = train(\n",
    "    max_epochs=100,\n",
    "    branch_numbers=[3, 6, 9],\n",
    "    head_type='resmlp',\n",
    "    dataset='disco',\n",
    "    version=5,\n",
    "    temporary=False\n",
    ")\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
