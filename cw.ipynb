{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [4, 5]\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "DISTRIBUTED_STRATEGY = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.NcclAllReduce(),\n",
    "    devices=['/gpu:%d' % index for index in range(len(SELECTED_GPUS))]\n",
    ")\n",
    "\n",
    "NUM_GPUS = DISTRIBUTED_STRATEGY.num_replicas_in_sync\n",
    "\n",
    "print('Number of devices: {}'.format(NUM_GPUS))\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from skimage import transform\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "from vit_keras import vit\n",
    "from vit_keras.layers import ClassToken, AddPositionEmbs, MultiHeadSelfAttention, TransformerBlock\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "PATCH_SIZE = 16\n",
    "NUM_PATCHES = (384 // PATCH_SIZE) ** 2 + 1\n",
    "HIDDEN_DIM = 768\n",
    "VIDEO_PATCHES = (2, 3)\n",
    "VIDEO_SIZE = (VIDEO_PATCHES[0] * IMAGE_SIZE, VIDEO_PATCHES[1] * IMAGE_SIZE)\n",
    "MLP_DIM = 3072  # ResMLP\n",
    "CHANNELS_MLP_DIM = 3072  # MLP-Mixer\n",
    "TOKENS_MLP_DIM = 384  # MLP-Mixer\n",
    "PRECOMPUTE_DIR = 'precompute'\n",
    "PRECOMPUTE_FASHION_MNIST_DIR = os.path.join(PRECOMPUTE_DIR, 'fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(model):\n",
    "    string_list = []\n",
    "    model.summary(print_fn=lambda x: string_list.append(x))\n",
    "    for string in string_list:\n",
    "        if string.startswith('Trainable params:'):\n",
    "            return int(string.split()[-1].replace(',', ''))\n",
    "    return None\n",
    "\n",
    "def get_flops(model):\n",
    "    \"\"\"\n",
    "    from https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-768977280\n",
    "    \"\"\"\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/leondgarse/Keras_mlp/blob/main/res_mlp.py\n",
    "\n",
    "def channel_affine(inputs, use_bias=True, weight_init_value=1, name=''):\n",
    "    ww_init = tfkeras.initializers.Constant(weight_init_value) if weight_init_value != 1 else 'ones'\n",
    "    nn = tf.keras.backend.expand_dims(inputs, 1)\n",
    "    nn = tf.keras.layers.DepthwiseConv2D(1, depthwise_initializer=ww_init, use_bias=use_bias, name=name + 'affine')(nn)\n",
    "    return tf.keras.backend.squeeze(nn, 1)\n",
    "\n",
    "def mlp_block(inputs, mlp_dim, activation='gelu', name=''):\n",
    "    affine_inputs = channel_affine(inputs, use_bias=True, name=name + '1_')\n",
    "    nn = tf.keras.layers.Permute((2, 1), name=name + 'permute_1')(affine_inputs)\n",
    "    nn = tf.keras.layers.Dense(nn.shape[-1], name=name + 'dense_1')(nn)\n",
    "    nn = tf.keras.layers.Permute((2, 1), name=name + 'permute_2')(nn)\n",
    "    nn = channel_affine(nn, use_bias=False, name=name + '1_gamma_')\n",
    "    skip_conn = tf.keras.layers.Add(name=name + 'add_1')([nn, affine_inputs])\n",
    "\n",
    "    affine_skip = channel_affine(skip_conn, use_bias=True, name=name + '2_')\n",
    "    nn = tf.keras.layers.Dense(mlp_dim, name=name + 'dense_2_1')(affine_skip)\n",
    "    nn = tf.keras.layers.Activation(activation, name=name + 'gelu')(nn)\n",
    "    nn = tf.keras.layers.Dense(inputs.shape[-1], name=name + 'dense_2_2')(nn)\n",
    "    nn = channel_affine(nn, use_bias=False, name=name + '2_gamma_')\n",
    "    nn = tf.keras.layers.Add(name=name + 'add_2')([nn, affine_skip])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/Benjamin-Etheredge/mlp-mixer-keras/blob/main/mlp_mixer_keras/mlp_mixer.py\n",
    "\n",
    "class MlpBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, hidden_dim, activation=None, **kwargs):\n",
    "        super(MlpBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = tf.keras.activations.gelu\n",
    "\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim)\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        self.dense2 = tf.keras.layers.Dense(dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_signature):\n",
    "        return (input_signature[0], self.dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MlpBlock, self).get_config().copy()\n",
    "        config.update({\n",
    "            'dim': self.dim,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MixerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        channel_dim,\n",
    "        token_mixer_hidden_dim,\n",
    "        channel_mixer_hidden_dim=None,\n",
    "        activation=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MixerBlock, self).__init__(**kwargs)\n",
    "\n",
    "        if activation is None:\n",
    "            activation = tf.keras.activations.gelu\n",
    "\n",
    "        if channel_mixer_hidden_dim is None:\n",
    "            channel_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.channel_dim = channel_dim\n",
    "        self.token_mixer_hidden_dim = token_mixer_hidden_dim\n",
    "        self.channel_mixer_hidden_dim = channel_mixer_hidden_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "        self.permute1 = tf.keras.layers.Permute((2, 1))\n",
    "        self.token_mixer = MlpBlock(num_patches, token_mixer_hidden_dim, name='token_mixer')\n",
    "\n",
    "        self.permute2 = tf.keras.layers.Permute((2, 1))\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(axis=1)\n",
    "        self.channel_mixer = MlpBlock(channel_dim, channel_mixer_hidden_dim, name='channel_mixer')\n",
    "\n",
    "        self.skip_connection1 = tf.keras.layers.Add()\n",
    "        self.skip_connection2 = tf.keras.layers.Add()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MixerBlock, self).get_config().copy()\n",
    "        config.update({\n",
    "            'num_patches': self.num_patches,\n",
    "            'channel_dim': self.channel_dim,\n",
    "            'token_mixer_hidden_dim': self.token_mixer_hidden_dim,\n",
    "            'channel_mixer_hidden_dim': self.channel_mixer_hidden_dim,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        skip_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.permute1(x)\n",
    "        x = self.token_mixer(x)\n",
    "\n",
    "        x = self.permute2(x)\n",
    "\n",
    "        x = self.skip_connection1([x, skip_x])\n",
    "        skip_x = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.channel_mixer(x)\n",
    "\n",
    "        x = self.skip_connection2([x, skip_x])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(branch_number, head_type):\n",
    "    model_input = tf.keras.Input(shape=(NUM_PATCHES, HIDDEN_DIM))\n",
    "    y = model_input\n",
    "    if head_type == 'resmlp':\n",
    "        y = mlp_block(y, mlp_dim=MLP_DIM, name='mlp_mixer')\n",
    "        y = tf.keras.layers.GlobalAveragePooling1D()(y)\n",
    "    elif head_type == 'mlp':\n",
    "        y = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6,\n",
    "            name='Transformer/encoder_norm_x'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x')(y)\n",
    "    elif head_type == 'vit':\n",
    "        y, _ = TransformerBlock(\n",
    "            num_heads=12,\n",
    "            mlp_dim=3072,\n",
    "            dropout=0.1,\n",
    "            name='Transformer/encoderblock_x'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6,\n",
    "            name='Transformer/encoder_norm_x'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x')(y)\n",
    "    elif head_type == 'cnn_ignore':\n",
    "        channels = HIDDEN_DIM\n",
    "        width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "        y = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken')(y)\n",
    "        y = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape')(y)\n",
    "        y = tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation='elu',\n",
    "            padding='same'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "        y = tf.keras.layers.Flatten()(y)\n",
    "    elif head_type == 'cnn_add':    \n",
    "        channels = HIDDEN_DIM\n",
    "        width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "\n",
    "        y1 = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken_x')(y)\n",
    "        y1 = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape')(y1)\n",
    "\n",
    "        y2 = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x')(y)\n",
    "        y2 = tf.keras.layers.RepeatVector(width * height)(y2)\n",
    "        y2 = tf.keras.layers.Reshape((width, height, channels), name='cls_reshape')(y2)\n",
    "\n",
    "        y = tf.keras.layers.Add()([y1, y2])\n",
    "\n",
    "        y = tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation='elu',\n",
    "            padding='same'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "        y = tf.keras.layers.Flatten()(y)\n",
    "    elif head_type == 'cnn_project':\n",
    "        channels = HIDDEN_DIM\n",
    "        width = height = IMAGE_SIZE // PATCH_SIZE\n",
    "\n",
    "        y1 = tf.keras.layers.Lambda(lambda v: v[:, 1:], name='RemoveToken_x')(y)\n",
    "        y1 = tf.keras.layers.Reshape((width, height, channels), name='cnn_reshape')(y1)\n",
    "\n",
    "        y2 = tf.keras.layers.Lambda(lambda v: v[:, 0], name='ExtractToken_x')(y)\n",
    "        y2 = tf.keras.layers.RepeatVector(width * height)(y2)\n",
    "        y2 = tf.keras.layers.Reshape((width, height, channels), name='cls_reshape')(y2)\n",
    "\n",
    "        y = tf.keras.layers.Concatenate()([y1, y2])\n",
    "\n",
    "        y = tf.keras.layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation='elu',\n",
    "            padding='same'\n",
    "        )(y)\n",
    "        y = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(y)\n",
    "        y = tf.keras.layers.Flatten()(y)\n",
    "    elif head_type == 'mlp_mixer':\n",
    "        num_patches = (IMAGE_SIZE // PATCH_SIZE) ** 2 + 1\n",
    "        y = MixerBlock(\n",
    "            num_patches=num_patches,\n",
    "            channel_dim=HIDDEN_DIM,\n",
    "            token_mixer_hidden_dim=TOKENS_MLP_DIM,\n",
    "            channel_mixer_hidden_dim=CHANNELS_MLP_DIM\n",
    "        )(y)\n",
    "        y = tf.keras.layers.GlobalAveragePooling1D()(y)\n",
    "\n",
    "    # MLP head\n",
    "    initializer = tf.keras.initializers.he_normal()\n",
    "    regularizer = tf.keras.regularizers.l2()\n",
    "    y = tf.keras.layers.Dense(\n",
    "        units=256,\n",
    "        activation='elu',\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=regularizer\n",
    "    )(y)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(\n",
    "        units=256,\n",
    "        activation='elu',\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=regularizer\n",
    "    )(y)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(\n",
    "        units=10,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=regularizer\n",
    "    )(y)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=model_input,\n",
    "        outputs=y\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, split, branch_number, batch_size):\n",
    "        self.split = split\n",
    "        self.branch_number = branch_number\n",
    "        self.batch_size = batch_size * NUM_GPUS\n",
    "        self.dir = PRECOMPUTE_FASHION_MNIST_DIR\n",
    "        self.count = sum([\n",
    "            1 if file_name.startswith('%s_branch%d_sample' % (\n",
    "                self.split,\n",
    "                self.branch_number\n",
    "            )) else 0 for file_name in os.listdir(self.dir)\n",
    "        ])\n",
    "        self.random_permutation = np.random.permutation(self.count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.count / self.batch_size)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.random_permutation = np.random.permutation(self.count)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = []\n",
    "        labels = []\n",
    "        for i in self.random_permutation[index * self.batch_size:(index + 1) * self.batch_size]:\n",
    "            cache_file_path = os.path.join(\n",
    "                self.dir,\n",
    "                '%s_branch%d_sample%d.pkl' % (self.split, self.branch_number, i)\n",
    "            )\n",
    "            with open(cache_file_path, 'rb') as cache_file:\n",
    "                contents = pickle.load(cache_file)\n",
    "                features.append(contents['features'])\n",
    "                labels.append(contents['label'])\n",
    "        return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epochs, branch_number, head_type, batch_size=64):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    with DISTRIBUTED_STRATEGY.scope():\n",
    "        model = get_model(branch_number, head_type)\n",
    "        branch_params = get_params(model) / 10 ** 6\n",
    "        total_flops = get_flops(model) / 10 ** 9\n",
    "\n",
    "    lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.6,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    save_model_checkpoint_file = 'vit_shtb_cw_%d_%s_head_precomputed_v1.h5' % (branch_number, head_type)\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        save_model_checkpoint_file,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1,\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        FashionMNISTSequence('train', branch_number, batch_size),\n",
    "        validation_data=FashionMNISTSequence('val', branch_number, batch_size),\n",
    "        epochs=max_epochs,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            lr_reduce,\n",
    "            early_stop,\n",
    "            checkpoint\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_accuracy = model.evaluate(FashionMNISTSequence('test', branch_number, batch_size))\n",
    "\n",
    "    return model, test_accuracy, branch_params, total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, results_path):\n",
    "    with open(results_path, 'w') as results_file:\n",
    "        results_file.write(json.dumps(results))\n",
    "\n",
    "def print_results(results_path):\n",
    "    with open(results_path, 'r') as results_file:\n",
    "        print(json.loads(results_file.read()))\n",
    "\n",
    "def get_results_path(head_type):\n",
    "    return 'shtb_%s.json' % head_type\n",
    "\n",
    "def run_experiment(head_type):\n",
    "    results = []\n",
    "    for i in reversed(range(1, 12)):\n",
    "        model, test_accuracy, branch_params, total_flops = train(100, i, head_type)        \n",
    "        results.append({\n",
    "            'exit': i,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'branch_params': branch_params,\n",
    "            'total_flops': total_flops,\n",
    "        })\n",
    "        results_path = get_results_path(head_type)\n",
    "        save_results(results, results_path)\n",
    "        print_results(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('vit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('resmlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('mlp_mixer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('cnn_ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('cnn_add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('cnn_project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
